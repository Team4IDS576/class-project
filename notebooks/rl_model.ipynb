{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# RoadNetEnv\n",
    "\n",
    "import gymnasium\n",
    "from gymnasium.spaces import Box, Dict, Discrete\n",
    "from gymnasium.spaces.utils import flatten_space\n",
    "from gymnasium.utils import EzPickle\n",
    "import numpy as np\n",
    "\n",
    "from pettingzoo import AECEnv\n",
    "from pettingzoo.utils import agent_selector, wrappers\n",
    "from pettingzoo.utils.conversions import parallel_wrapper_fn\n",
    "\n",
    "\n",
    "# from NguyenNetwork import nguyenNetwork, traffic, latency\n",
    "\n",
    "# allows to import the parallel environment using \"from NguyenNetworkEnv import parallel_env\"\n",
    "__all__ = [\"ManualPolicy\", \"env\", \"parallel_env\", \"raw_env\"]\n",
    "\n",
    "# environment wrapper\n",
    "def env(**kwargs):\n",
    "    env = raw_env(**kwargs)\n",
    "    env = wrappers.OrderEnforcingWrapper(env)\n",
    "    return env\n",
    "\n",
    "# AEC to parallel wrapper\n",
    "parallel_env = parallel_wrapper_fn(env)\n",
    "class raw_env(AECEnv):\n",
    "    metadata = {\n",
    "        \"name\": \"NguyenNet\",\n",
    "        \"is_parallelizable\": True\n",
    "    }\n",
    "    \n",
    "    \"\"\"\n",
    "    This is the traffic assignment environment. More documentation to follow.\n",
    "    \"\"\"\n",
    "    \n",
    "    # initialize environment with the Nguyen Network and Traffic Demand from agents\n",
    "    def __init__(\n",
    "        self,\n",
    "        net = nguyenNetwork(),\n",
    "        traffic = traffic_f(high=False),\n",
    "        render_mode = None\n",
    "        ):\n",
    "        \n",
    "        # no rendering at the moment\n",
    "        self.render_mode = render_mode\n",
    "        \n",
    "        # initialize network from Nguyen Network\n",
    "        self.road_network = net\n",
    "        self.traffic = traffic\n",
    "        \n",
    "        # initialize agents\n",
    "        self.agents = self.traffic[\"agents\"] # list of agents in environment\n",
    "        self.possible_agents = self.agents[:]\n",
    "        self.agent_name_mapping = dict(zip(self.agents, list(range(len(self.agents)))))\n",
    "        self._agent_selector = agent_selector(self.agents)\n",
    "        \n",
    "        # check\n",
    "        self.terminations_check = dict(zip(self.agents, [False for _ in self.agents]))\n",
    "        self.truncations_check = dict(zip(self.agents, [False for _ in self.agents]))\n",
    "        self.termination_trigger = dict(zip(self.agents, [False for _ in self.agents]))\n",
    "        self.truncation_trigger = dict(zip(self.agents, [False for _ in self.agents]))\n",
    "        \n",
    "        # agent origin, destination, and location information\n",
    "        self.agent_origins = self.traffic[\"origins\"].copy()\n",
    "        self.agent_origin_backup = self.traffic[\"origins\"].copy()\n",
    "        self.agent_locations = self.traffic[\"origins\"].copy()\n",
    "        self.agent_destinations = self.traffic[\"destinations\"]\n",
    "        self.agent_path_histories = {agent: [location] for agent, location in zip(self.agents, self.agent_locations)}\n",
    "        self.agent_wait_time = {agent: 0 for agent in self.agents}\n",
    "        self.agent_travel_time = {agent: 0 for agent in self.agents}\n",
    "\n",
    "        # agent unflattened observation space, this is flattened alphabetically btw.\n",
    "        self.unflattened_observation_spaces = {\n",
    "            agent: Dict({\n",
    "                \"observation\": Box(low=-1, high=12, shape=(2,1), dtype=int),\n",
    "                \"latencies\": Box(low=0, high=30, shape=(2,1), dtype=int),\n",
    "                \"location\": Box(low = 0, high = 12, shape = (1,1), dtype=int),\n",
    "            }) for agent in self.agents\n",
    "        }\n",
    "        \n",
    "        # agent flattened observatino space\n",
    "        self.observation_spaces = {\n",
    "            i: flatten_space(self.unflattened_observation_spaces[i]) for i in self.unflattened_observation_spaces\n",
    "        }\n",
    "        \n",
    "        # agent action space\n",
    "        self.action_spaces = dict(\n",
    "            # with the nguyen network agents have at most 2 choices\n",
    "            zip(\n",
    "                self.agents,\n",
    "                [gymnasium.spaces.Discrete(2)]*len(self.agents)\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # agent terminal and truncated states\n",
    "        self.terminate = False\n",
    "        self.truncate = False\n",
    "        \n",
    "    def observation_space(self, agent):\n",
    "        return self.observation_spaces[agent]\n",
    "    \n",
    "    def action_space(self, agent):\n",
    "        return self.action_spaces[agent]\n",
    "    \n",
    "    def observe(self, agent):\n",
    "\n",
    "        agent_idx = self.agent_name_mapping[agent]\n",
    "        \n",
    "        # get possible nodes the agent can travel to\n",
    "        agent_position = self.agent_locations[agent_idx]\n",
    "        agent_node_neighbors = list(self.road_network.neighbors(agent_position))\n",
    "        \n",
    "        # encode position\n",
    "        position = []\n",
    "        encoding = int(self.agent_locations[agent_idx])-1\n",
    "        position.append(encoding)\n",
    "        \n",
    "        # encode node positions\n",
    "        node_encoded = []\n",
    "        for node in agent_node_neighbors:\n",
    "            encoding = int(node)-1\n",
    "            node_encoded.append(encoding)\n",
    "        \n",
    "        # currently using ffs attribute, need to revise to use updated latency\n",
    "        # neighboring_nodes_ffs = []\n",
    "        # for node in agent_node_neighbors:\n",
    "        #     ffs_value = self.road_network.get_edge_data(agent_position, node)[\"ffs\"]\n",
    "        #     neighboring_nodes_ffs.append(ffs_value)\n",
    "\n",
    "        # updated to use \"latency\" instead of \"ffs\"\n",
    "        neighboring_nodes_latencies = []\n",
    "        for node in agent_node_neighbors:\n",
    "            latency_value = self.road_network.get_edge_data(agent_position, node)[\"latency\"]\n",
    "            neighboring_nodes_latencies.append(latency_value)\n",
    "\n",
    "        # return observation â€“ a list in structured as [node1, latency1, node 2, latency2]\n",
    "        if len(agent_node_neighbors) == 1:\n",
    "            node_encoded = node_encoded*2\n",
    "            # neighboring_nodes_ffs = neighboring_nodes_ffs*2\n",
    "            neighboring_nodes_latencies = neighboring_nodes_latencies*2\n",
    "        if len(agent_node_neighbors) == 0:\n",
    "            node_encoded = [-1,-1]\n",
    "            # neighboring_nodes_ffs = [0,0]\n",
    "            neighboring_nodes_latencies = [0,0]\n",
    "            \n",
    "        # observations  = np.array(neighboring_nodes_ffs+position+node_encoded)\n",
    "        observations = np.array(neighboring_nodes_latencies + position + node_encoded)\n",
    "        \n",
    "        return observations\n",
    "\n",
    "\n",
    "\n",
    "    def state(self) -> np.ndarray:\n",
    "        \"\"\"We need to return an np-array like object for logging\"\"\"\n",
    "        return list(self.termination_trigger.values()), list(self.agent_travel_time.values())\n",
    "\n",
    "\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"This does not work\"\"\"\n",
    "        # # check if agent is dead\n",
    "        # if (\n",
    "        #     self.terminations_check[self.agent_selection] or\n",
    "        #     self.truncations_check[self.agent_selection]\n",
    "        # ):\n",
    "        #     # self.rewards[self.agent_selection] = 0\n",
    "        #     self.agent_selection = self._agent_selector.next()\n",
    "        #     #self._accumulate_rewards()\n",
    "        #     return\n",
    "        \n",
    "        action = np.asarray(action)\n",
    "        agent = self.agent_selection\n",
    "        agent_idx = self.agent_name_mapping[agent]        \n",
    "\n",
    "        self._cumulative_rewards[agent] = 0\n",
    "        \n",
    "        if self._agent_selector.is_last():\n",
    "            # Track the count of agents on each link for all agents' positions\n",
    "            agents_on_link = {edge: 0 for edge in self.road_network.edges()}\n",
    "            # print(agents_on_link)\n",
    "            # print(self.road_network.edges())\n",
    "\n",
    "            # Update the count based on all agents' neighbors\n",
    "            for current_agent in self.agents:                   \n",
    "                agent_idx = self.agent_name_mapping[current_agent]                \n",
    "                agent_position = self.agent_locations[agent_idx]                \n",
    "                agent_node_neighbors = list(self.road_network.neighbors(agent_position))\n",
    "\n",
    "                for node in agent_node_neighbors:                    \n",
    "                    for edge in zip([agent_position], [node]):                        \n",
    "                        agents_on_link[edge] += 1  # Increment count when an agent enters a link                                  \n",
    "\n",
    "            for edge, agent_count in agents_on_link.items():                \n",
    "                link_data = self.road_network.get_edge_data(*edge)  \n",
    "                # print(link_data) # not updated                                       \n",
    "\n",
    "                flow = agent_count  # 'flow' takes the total number of vehicles in the link                                         \n",
    "                start_node = np.int64(edge[0])            \n",
    "                end_node = np.int64(edge[1])            \n",
    "\n",
    "                new_latency = latency(flow, start_node, end_node)                       \n",
    "                link_data[\"latency\"] = new_latency            \n",
    "                # print(new_latency)                   \n",
    "\n",
    "                self.road_network[edge[0]][edge[1]][\"latency\"] = new_latency\n",
    "                # print(f\"Updated latency for edge {edge}: {self.road_network[edge[0]][edge[1]]['latency']}\") \n",
    "\n",
    "            self._clear_rewards()\n",
    "        else:\n",
    "            self._clear_rewards()\n",
    "            pass\n",
    "\n",
    "        # agent travel decrement\n",
    "        if self.agent_wait_time[agent] != 0:\n",
    "            # if agent has waiting time (i.e. \"traveling\" along edge, decrement wait time by one time step)\n",
    "            self.agent_wait_time[agent] -= 1\n",
    "            self.agent_selection = self._agent_selector.next()\n",
    "            self._accumulate_rewards()\n",
    "            return\n",
    "        \n",
    "        # agent reaches terminal state\n",
    "        if self.agent_locations[agent_idx] == self.agent_destinations[agent_idx]:\n",
    "            self.terminations_check[agent] = True\n",
    "            \n",
    "            # return reward for arriving at destionation\n",
    "            if self.termination_trigger[agent] == False:\n",
    "                completion_reward = 100\n",
    "                self.termination_trigger[agent] = True\n",
    "            else:\n",
    "                completion_reward = 0\n",
    "            \n",
    "            self.rewards[agent] = completion_reward\n",
    "            self.agent_selection = self._agent_selector.next()\n",
    "            self._accumulate_rewards()\n",
    "            return\n",
    "        \n",
    "        # agent reaches truncation state\n",
    "        if self.agent_locations[agent_idx] != self.agent_destinations[agent_idx] and \\\n",
    "        (self.agent_locations[agent_idx] == \"2\" or self.agent_locations[agent_idx] == \"3\"):\n",
    "            self.truncations_check[agent] = True\n",
    "                    \n",
    "            # return penalty for arriving at wrong destination\n",
    "            if self.truncation_trigger[agent] == False:\n",
    "                completion_penalty = -100\n",
    "                self.truncation_trigger[agent] = True\n",
    "            else:\n",
    "                completion_penalty = 0\n",
    "                \n",
    "            self.rewards[agent] = completion_penalty\n",
    "            self.agent_selection = self._agent_selector.next()\n",
    "            self._accumulate_rewards()\n",
    "            return\n",
    "        \n",
    "        # agent chooses action\n",
    "        choices =  list(\n",
    "            self.road_network.neighbors(\n",
    "                self.agent_locations[agent_idx]\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # if only one route\n",
    "        if len(choices) == 1:\n",
    "            choices = [choices[0], choices[0]]\n",
    "        \n",
    "        # select action\n",
    "        chosen_route = choices[action]\n",
    "\n",
    "        # reward based on chosen route latency, again using ffs instead of calculated latency, need a _calculate_reward(agent) method for this\n",
    "        reward = self.road_network.get_edge_data(\n",
    "            self.agent_locations[agent_idx],\n",
    "            # chosen_route)[\"ffs\"]\n",
    "            chosen_route)[\"latency\"]\n",
    "        \n",
    "        # add negative latency to reward â€“ DQN to maximize negative reward\n",
    "        self.rewards[agent] = -1*reward\n",
    "        \n",
    "        # update latency\n",
    "        self.agent_wait_time[agent] += reward\n",
    "        self.agent_travel_time[agent] += reward        \n",
    "        \n",
    "        # update agent position\n",
    "        self.agent_locations[agent_idx] = chosen_route\n",
    "        \n",
    "        # update path history\n",
    "        self.agent_path_histories[agent].append(chosen_route)\n",
    "        \n",
    "        \n",
    "        # set the next agent to act\n",
    "        self.agent_selection = self._agent_selector.next()\n",
    "        self._accumulate_rewards()\n",
    "\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        # reset to initial states\n",
    "        self.agent_origins = self.agent_origin_backup.copy()\n",
    "        self.agent_locations = self.agent_origin_backup.copy()\n",
    "        self.agent_path_histories = {agent: [location] for agent, location in zip(self.agents, self.agent_origins)}\n",
    "        self.agent_wait_time = {agent: 0 for agent in self.agents}\n",
    "        self.agent_travel_time = {agent: 0 for agent in self.agents} # added\n",
    "\n",
    "        self.agents = self.possible_agents[:]        \n",
    "        self._agent_selector.reinit(self.agents)\n",
    "        self.agent_selection = self._agent_selector.next()\n",
    "        self.terminate = False\n",
    "        self.truncate = False\n",
    "        self.rewards = dict(zip(self.agents, [0 for _ in self.agents]))\n",
    "        self._cumulative_rewards = {a: 0 for a in self.agents}\n",
    "        self.terminations = dict(zip(self.agents, [False for _ in self.agents]))\n",
    "        self.truncations = dict(zip(self.agents, [False for _ in self.agents]))\n",
    "        self.infos = dict(zip(self.agents, [{} for _ in self.agents]))\n",
    "        \n",
    "        # Reset termination and truncation checks\n",
    "        # check\n",
    "        self.terminations_check = dict(zip(self.agents, [False for _ in self.agents]))\n",
    "        self.truncations_check = dict(zip(self.agents, [False for _ in self.agents]))\n",
    "        self.termination_trigger = dict(zip(self.agents, [False for _ in self.agents]))\n",
    "        self.truncation_trigger = dict(zip(self.agents, [False for _ in self.agents]))\n",
    "        \n",
    "        # Reset road network-related variables\n",
    "        # print(\"Resetting latency values\")\n",
    "        for edge in self.road_network.edges():\n",
    "            start_node, end_node = edge\n",
    "            self.road_network[start_node][end_node][\"latency\"] = 0            \n",
    "\n",
    "        # Clear any existing rewards\n",
    "        #self._clear_rewards()\n",
    "        # print(self.rewards)\n",
    "        # print(self.state())\n",
    "\n",
    "        # Return the initial state\n",
    "        #return self.state()\n",
    "        \n",
    "\n",
    "train_agileRL\n",
    "# import RoadNetEnv\n",
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "from agilerl.components.multi_agent_replay_buffer import MultiAgentReplayBuffer\n",
    "from agilerl.algorithms.maddpg import MADDPG\n",
    "\n",
    "# instantiate env and torch device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# env = RoadNetEnv.parallel_env()\n",
    "env = parallel_env()\n",
    "env.reset()\n",
    "\n",
    "# configure algo input parameters\n",
    "try:\n",
    "    state_dim = [env.observation_space(agent).n for agent in env.agents]\n",
    "    one_hot = True\n",
    "except Exception:\n",
    "    state_dim = [env.observation_space(agent).shape for agent in env.agents]\n",
    "    one_hot = False\n",
    "try:\n",
    "    action_dim = [env.action_space(agent).n for agent in env.agents]\n",
    "    discrete_actions = True\n",
    "    max_action = None\n",
    "    min_action = None\n",
    "except Exception:\n",
    "    action_dim = [env.action_space(agent).shape[0] for agent in env.agents]\n",
    "    discrete_actions = False\n",
    "    max_action = [env.action_space(agent).high for agent in env.agents]\n",
    "    min_action = [env.action_space(agent).low for agent in env.agents]\n",
    "\n",
    "n_agents = env.num_agents\n",
    "agent_ids = [agent_id for agent_id in env.agents]\n",
    "done = {agent_id: False for agent_id in env.agents}\n",
    "field_names = [\"state\", \"action\", \"reward\", \"next_state\", \"done\"]\n",
    "memory = MultiAgentReplayBuffer(\n",
    "    memory_size=1000000,\n",
    "    field_names=field_names,\n",
    "    agent_ids=agent_ids,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "with open('device_info.txt', 'w') as file:\n",
    "\tfile.write(f\"Device: {device}\")\n",
    "\n",
    "NET_CONFIG = {\n",
    "    \"arch\": \"mlp\",\n",
    "    \"h_size\": [128, 128]\n",
    "}\n",
    "\n",
    "agent = MADDPG(\n",
    "    state_dims=state_dim,\n",
    "    action_dims=action_dim,\n",
    "    one_hot=one_hot,\n",
    "    n_agents=n_agents,\n",
    "    agent_ids=agent_ids,\n",
    "    max_action=max_action,\n",
    "    min_action=min_action,\n",
    "    discrete_actions=True,\n",
    "    device=device,\n",
    "    net_config=NET_CONFIG,\n",
    "    lr=1e-2,\n",
    "    batch_size=8\n",
    ")\n",
    "\n",
    "# load checkpoint\n",
    "# agent.loadCheckpoint(\"./checkpoint/checkpoint_50.pt\")\n",
    "\n",
    "episodes = 400\n",
    "max_steps = 50\n",
    "epsilon = 1.0\n",
    "eps_end = 0.1\n",
    "eps_decay = 0.995\n",
    "\n",
    "episode_travel_times = []\n",
    "\n",
    "log_interval = 10\n",
    "start_ep = 51\n",
    "\n",
    "for ep in range(episodes):\n",
    "    state, info = env.reset()\n",
    "    agent_reward = {agent_id: 0 for agent_id in env.agents}\n",
    "    \n",
    "    for i in range(max_steps):\n",
    "        agent_mask = info[\"agent_mask\"] if \"agent_mask\" in info.keys() else None\n",
    "        env_defined_actions = (\n",
    "            info[\"env_defined_actions\"]\n",
    "            if \"env_defined_actions\" in info.keys()\n",
    "            else None\n",
    "        )\n",
    "        \n",
    "        # get next action from agent\n",
    "        cont_actions, discrete_action = agent.getAction(\n",
    "            state, epsilon, agent_mask, env_defined_actions\n",
    "        )\n",
    "        if agent.discrete_actions:\n",
    "            action = discrete_action\n",
    "        else:\n",
    "            action = cont_actions\n",
    "        \n",
    "        # act in environment\n",
    "        next_state, reward, termination, truncation, info = env.step(\n",
    "            action\n",
    "        )\n",
    "        \n",
    "        # save experience to replay buffer\n",
    "        memory.save2memory(state, cont_actions, reward, next_state, done)\n",
    "        \n",
    "        for agent_id, r in reward.items():\n",
    "            agent_reward[agent_id] += r\n",
    "        \n",
    "        # learn according to learning frequency\n",
    "        if (memory.counter % agent.learn_step == 0) and (len(memory) >= agent.batch_size):\n",
    "            experiences = memory.sample(agent.batch_size)\n",
    "            agent.learn(experiences)\n",
    "            \n",
    "        # update state\n",
    "        state = next_state\n",
    "\n",
    "    # metric logging\n",
    "    finishers, travel_time = env.state()\n",
    "    episode_travel_times.append(travel_time) # export to csv\n",
    "    \n",
    "    # save the total episode reward\n",
    "    score = sum(agent_reward.values())\n",
    "    agent.scores.append(score)\n",
    "    \n",
    "    \n",
    "    # update epsilon for exploration\n",
    "    epsilon = max(eps_end, epsilon * eps_decay)\n",
    "\n",
    "    if (ep+start_ep) % log_interval == 0 or ep == episodes -1:\n",
    "        reward_scores = pd.DataFrame(agent.scores)\n",
    "        reward_scores.to_csv(f\"./results/reward_{ep+start_ep}.csv\")\n",
    "        travel_times = pd.DataFrame(episode_travel_times)\n",
    "        travel_times.to_csv(f\"./results/travel_times_{ep+start_ep}.csv\")\n",
    "        agent.saveCheckpoint(f\"./checkpoint/checkpoint_{ep+start_ep}.pt\")\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NguyenNetwork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'NguyenLowDemand50.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/arumi/class-project-3/notebooks/rl_model.ipynb Cell 3\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/arumi/class-project-3/notebooks/rl_model.ipynb#W1sZmlsZQ%3D%3D?line=103'>104</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m traffic\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/arumi/class-project-3/notebooks/rl_model.ipynb#W1sZmlsZQ%3D%3D?line=107'>108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m__main__\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/arumi/class-project-3/notebooks/rl_model.ipynb#W1sZmlsZQ%3D%3D?line=108'>109</a>\u001b[0m     traffic \u001b[39m=\u001b[39m traffic_f(high\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/arumi/class-project-3/notebooks/rl_model.ipynb#W1sZmlsZQ%3D%3D?line=109'>110</a>\u001b[0m     \u001b[39mprint\u001b[39m(traffic)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/arumi/class-project-3/notebooks/rl_model.ipynb#W1sZmlsZQ%3D%3D?line=110'>111</a>\u001b[0m RoadNetEnv\n",
      "\u001b[1;32m/Users/arumi/class-project-3/notebooks/rl_model.ipynb Cell 3\u001b[0m line \u001b[0;36m8\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/arumi/class-project-3/notebooks/rl_model.ipynb#W1sZmlsZQ%3D%3D?line=79'>80</a>\u001b[0m     demand \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(\u001b[39m\"\u001b[39m\u001b[39mNguyenDemandHighDemand200.csv\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/arumi/class-project-3/notebooks/rl_model.ipynb#W1sZmlsZQ%3D%3D?line=80'>81</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/arumi/class-project-3/notebooks/rl_model.ipynb#W1sZmlsZQ%3D%3D?line=81'>82</a>\u001b[0m     demand \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_csv(\u001b[39m\"\u001b[39;49m\u001b[39mNguyenLowDemand50.csv\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/arumi/class-project-3/notebooks/rl_model.ipynb#W1sZmlsZQ%3D%3D?line=82'>83</a>\u001b[0m agents \u001b[39m=\u001b[39m []\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/arumi/class-project-3/notebooks/rl_model.ipynb#W1sZmlsZQ%3D%3D?line=83'>84</a>\u001b[0m origins \u001b[39m=\u001b[39m []\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/rllib/lib/python3.11/site-packages/pandas/io/parsers/readers.py:948\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    944\u001b[0m     dtype_backend\u001b[39m=\u001b[39mdtype_backend,\n\u001b[1;32m    945\u001b[0m )\n\u001b[1;32m    946\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 948\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/rllib/lib/python3.11/site-packages/pandas/io/parsers/readers.py:611\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    608\u001b[0m _validate_names(kwds\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mnames\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[1;32m    610\u001b[0m \u001b[39m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 611\u001b[0m parser \u001b[39m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    613\u001b[0m \u001b[39mif\u001b[39;00m chunksize \u001b[39mor\u001b[39;00m iterator:\n\u001b[1;32m    614\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/rllib/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1448\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1445\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m kwds[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m   1447\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles: IOHandles \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 1448\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_engine(f, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mengine)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/rllib/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1705\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1703\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m mode:\n\u001b[1;32m   1704\u001b[0m         mode \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m-> 1705\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39m=\u001b[39m get_handle(\n\u001b[1;32m   1706\u001b[0m     f,\n\u001b[1;32m   1707\u001b[0m     mode,\n\u001b[1;32m   1708\u001b[0m     encoding\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1709\u001b[0m     compression\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mcompression\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1710\u001b[0m     memory_map\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mmemory_map\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mFalse\u001b[39;49;00m),\n\u001b[1;32m   1711\u001b[0m     is_text\u001b[39m=\u001b[39;49mis_text,\n\u001b[1;32m   1712\u001b[0m     errors\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding_errors\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mstrict\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m   1713\u001b[0m     storage_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mstorage_options\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1714\u001b[0m )\n\u001b[1;32m   1715\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1716\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles\u001b[39m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/rllib/lib/python3.11/site-packages/pandas/io/common.py:863\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    858\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(handle, \u001b[39mstr\u001b[39m):\n\u001b[1;32m    859\u001b[0m     \u001b[39m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    860\u001b[0m     \u001b[39m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    861\u001b[0m     \u001b[39mif\u001b[39;00m ioargs\u001b[39m.\u001b[39mencoding \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m ioargs\u001b[39m.\u001b[39mmode:\n\u001b[1;32m    862\u001b[0m         \u001b[39m# Encoding\u001b[39;00m\n\u001b[0;32m--> 863\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(\n\u001b[1;32m    864\u001b[0m             handle,\n\u001b[1;32m    865\u001b[0m             ioargs\u001b[39m.\u001b[39;49mmode,\n\u001b[1;32m    866\u001b[0m             encoding\u001b[39m=\u001b[39;49mioargs\u001b[39m.\u001b[39;49mencoding,\n\u001b[1;32m    867\u001b[0m             errors\u001b[39m=\u001b[39;49merrors,\n\u001b[1;32m    868\u001b[0m             newline\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    869\u001b[0m         )\n\u001b[1;32m    870\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    871\u001b[0m         \u001b[39m# Binary mode\u001b[39;00m\n\u001b[1;32m    872\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(handle, ioargs\u001b[39m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'NguyenLowDemand50.csv'"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "import pandas as pd\n",
    "\n",
    "def nguyenNetwork(high=True):\n",
    "    \n",
    "    # Read the CSV file into a DataFrame\n",
    "    data_types = {\"start node\": str, \"end node\": str} # node names should be str\n",
    "    \n",
    "    # for mac OS\n",
    "    if high == True:\n",
    "        links = pd.read_csv(\"./network/NguyenLinksHighDemand200.csv\", dtype=data_types)\n",
    "    else:\n",
    "        links = pd.read_csv(\"./network/NguyenLinksLowDemand50.csv\", dtype=data_types)\n",
    "    \n",
    "    links['flow'] = 0\n",
    "    \n",
    "    # instantiate null directed graph\n",
    "    network = nx.DiGraph()\n",
    "    \n",
    "    # initialize intersections\n",
    "    intersections = {\n",
    "        \"1\": {\"pos\": (1, 3)},\n",
    "        \"2\": {\"pos\": (4, 1)},\n",
    "        \"3\": {\"pos\": (3, 0)},\n",
    "        \"4\": {\"pos\": (0, 2)},\n",
    "        \"5\": {\"pos\": (1, 2)},\n",
    "        \"6\": {\"pos\": (2, 2)},\n",
    "        \"7\": {\"pos\": (3, 2)},\n",
    "        \"8\": {\"pos\": (4, 2)},\n",
    "        \"9\": {\"pos\": (1, 1)},\n",
    "        \"10\": {\"pos\": (2, 1)},\n",
    "        \"11\": {\"pos\": (3, 1)},\n",
    "        \"12\": {\"pos\": (2, 3)},\n",
    "        \"13\": {\"pos\": (2, 0)}\n",
    "    }\n",
    "\n",
    "    # add intersections\n",
    "    for node, attrs in intersections.items():\n",
    "        network.add_node(node, **attrs)\n",
    "    \n",
    "    # intialize roads\n",
    "\n",
    "    # Create a list of tuples from the DataFrame\n",
    "    roads = [(row[\"start node\"], \n",
    "              row[\"end node\"], \n",
    "              {\"ffs\": row[\"free flow speed\"], \n",
    "               \"capacity\": row[\"capacity\"],\n",
    "               \"alpha\": row[\"alpha\"],\n",
    "               \"beta\": row[\"beta\"],\n",
    "               \"latency\": row[\"latency\"],\n",
    "               \"flow\": row[\"flow\"]}) for _, row in links.iterrows()]\n",
    "\n",
    "    # Print the list of roads\n",
    "    # print(roads)\n",
    "\n",
    "\n",
    "    # add roads\n",
    "    network.add_edges_from(roads)\n",
    "    \n",
    "    return network\n",
    "\n",
    "def latency(flow, start_node, end_node):\n",
    "    network = nguyenNetwork()\n",
    "    edge_data = network.get_edge_data(str(start_node), str(end_node))\n",
    "    \n",
    "    # 'flow' takes the total No. of vehicle in the link\n",
    "    # 'start_node' and 'end_node' should be integer values\n",
    "    # This function assumes that the travel time of the links are BPR functions.\n",
    "    # for more information about BPR functions read p#358 of Sheffi's book.\n",
    "    c = edge_data['capacity']\n",
    "    t_0 = edge_data['ffs']\n",
    "    a = edge_data['alpha']\n",
    "    b = edge_data['beta']\n",
    "    t_link = t_0 * (1 + (a * ((flow/c) ** b)))\n",
    "    return round(t_link) # this function only returns integer values\n",
    "\n",
    "\n",
    "def traffic_f(high=True):\n",
    "    if high == True:\n",
    "        demand = pd.read_csv(\"NguyenDemandHighDemand200.csv\")\n",
    "    else:\n",
    "        demand = pd.read_csv(\"NguyenLowDemand50.csv\")\n",
    "    agents = []\n",
    "    origins = []\n",
    "    destinations = []\n",
    "    agent_no = 0\n",
    "\n",
    "    for index, row in demand.iterrows():\n",
    "        origin = str(row['Origin'])\n",
    "        destination = str(row['Destination'])\n",
    "        count = int(row['OD demand'])\n",
    "        for i in range(count):\n",
    "            agent_no += 1\n",
    "            agents.append(f'agent_{agent_no}')\n",
    "            origins.append(origin)\n",
    "            destinations.append(destination)\n",
    "\n",
    "    traffic = {\n",
    "        \"agents\": agents,\n",
    "        \"origins\": origins,\n",
    "        \"destinations\": destinations\n",
    "    }\n",
    "\n",
    "    return traffic\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    traffic = traffic_f(high=False)\n",
    "    print(traffic)\n",
    "RoadNetEnv"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rllib",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
