{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nguyen Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code defines a simulation framework for a transportation network using the NetworkX library. \n",
    "The network is based on the Nguyen network model, which includes intersections, roads, and traffic flow. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nguyenNetwork\n",
    "\n",
    "* Reads road network data from CSV files, creates a directed graph representing intersections and roads, and initializes road attributes.\n",
    "* Returns a NetworkX DiGraph representing the Nguyen network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import pandas as pd\n",
    "\n",
    "def nguyenNetwork(high=True):\n",
    "    \n",
    "    # Read the CSV file into a DataFrame\n",
    "    data_types = {\"start node\": str, \"end node\": str} # node names should be str\n",
    "    \n",
    "    # for mac OS\n",
    "    if high == True:\n",
    "        links = pd.read_csv(\"./network/NguyenLinksHighDemand200.csv\", dtype=data_types)\n",
    "    else:\n",
    "        links = pd.read_csv(\"./network/NguyenLinksLowDemand50.csv\", dtype=data_types)\n",
    "    \n",
    "    links['flow'] = 0\n",
    "    \n",
    "    # instantiate null directed graph\n",
    "    network = nx.DiGraph()\n",
    "    \n",
    "    # initialize intersections\n",
    "    intersections = {\n",
    "        \"1\": {\"pos\": (1, 3)},\n",
    "        \"2\": {\"pos\": (4, 1)},\n",
    "        \"3\": {\"pos\": (3, 0)},\n",
    "        \"4\": {\"pos\": (0, 2)},\n",
    "        \"5\": {\"pos\": (1, 2)},\n",
    "        \"6\": {\"pos\": (2, 2)},\n",
    "        \"7\": {\"pos\": (3, 2)},\n",
    "        \"8\": {\"pos\": (4, 2)},\n",
    "        \"9\": {\"pos\": (1, 1)},\n",
    "        \"10\": {\"pos\": (2, 1)},\n",
    "        \"11\": {\"pos\": (3, 1)},\n",
    "        \"12\": {\"pos\": (2, 3)},\n",
    "        \"13\": {\"pos\": (2, 0)}\n",
    "    }\n",
    "\n",
    "    # add intersections\n",
    "    for node, attrs in intersections.items():\n",
    "        network.add_node(node, **attrs)\n",
    "    \n",
    "    # intialize roads\n",
    "\n",
    "    # Create a list of tuples from the DataFrame\n",
    "    roads = [(row[\"start node\"], \n",
    "              row[\"end node\"], \n",
    "              {\"ffs\": row[\"free flow speed\"], \n",
    "               \"capacity\": row[\"capacity\"],\n",
    "               \"alpha\": row[\"alpha\"],\n",
    "               \"beta\": row[\"beta\"],\n",
    "               \"latency\": row[\"latency\"],\n",
    "               \"flow\": row[\"flow\"]}) for _, row in links.iterrows()]\n",
    "\n",
    "    # Print the list of roads\n",
    "    # print(roads)\n",
    "\n",
    "\n",
    "    # add roads\n",
    "    network.add_edges_from(roads)\n",
    "    \n",
    "    return network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## latency\n",
    "\n",
    "* Calculates the latency (travel time) on a road segment based on the Bureau of Public Roads (BPR) function, considering flow, capacity, and other parameters.\n",
    "* Parameters:\n",
    "  - flow: Total number of vehicles on the road.\n",
    "  - start_node and end_node: Identifiers for the start and end nodes of the road.\n",
    "* Returns the rounded travel time for the given road segment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def latency(flow, start_node, end_node):\n",
    "    network = nguyenNetwork()\n",
    "    edge_data = network.get_edge_data(str(start_node), str(end_node))\n",
    "    \n",
    "    # 'flow' takes the total No. of vehicle in the link\n",
    "    # 'start_node' and 'end_node' should be integer values\n",
    "    # This function assumes that the travel time of the links are BPR functions.\n",
    "    # for more information about BPR functions read p#358 of Sheffi's book.\n",
    "    c = edge_data['capacity']\n",
    "    t_0 = edge_data['ffs']\n",
    "    a = edge_data['alpha']\n",
    "    b = edge_data['beta']\n",
    "    t_link = t_0 * (1 + (a * ((flow/c) ** b)))\n",
    "    return round(t_link) # this function only returns integer values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## traffic_f\n",
    "\n",
    "* Generates synthetic traffic demand based on CSV files, creating a list of agents with origins and destinations.\n",
    "* Returns a dictionary containing information about agents, their origins, and destinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def traffic_f(high=True):\n",
    "    if high == True:\n",
    "        demand = pd.read_csv(\"./network/NguyenDemandHighDemand200.csv\")\n",
    "    else:\n",
    "        demand = pd.read_csv(\"./network/NguyenLowDemand50.csv\")\n",
    "    agents = []\n",
    "    origins = []\n",
    "    destinations = []\n",
    "    agent_no = 0\n",
    "\n",
    "    for index, row in demand.iterrows():\n",
    "        origin = str(row['Origin'])\n",
    "        destination = str(row['Destination'])\n",
    "        count = int(row['OD demand'])\n",
    "        for i in range(count):\n",
    "            agent_no += 1\n",
    "            agents.append(f'agent_{agent_no}')\n",
    "            origins.append(origin)\n",
    "            destinations.append(destination)\n",
    "\n",
    "    traffic = {\n",
    "        \"agents\": agents,\n",
    "        \"origins\": origins,\n",
    "        \"destinations\": destinations\n",
    "    }\n",
    "\n",
    "    return traffic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'agents': ['agent_1', 'agent_2', 'agent_3', 'agent_4', 'agent_5', 'agent_6', 'agent_7', 'agent_8', 'agent_9', 'agent_10', 'agent_11', 'agent_12', 'agent_13', 'agent_14', 'agent_15', 'agent_16', 'agent_17', 'agent_18', 'agent_19', 'agent_20', 'agent_21', 'agent_22', 'agent_23', 'agent_24', 'agent_25', 'agent_26', 'agent_27', 'agent_28', 'agent_29', 'agent_30', 'agent_31', 'agent_32', 'agent_33', 'agent_34', 'agent_35', 'agent_36', 'agent_37', 'agent_38', 'agent_39', 'agent_40', 'agent_41', 'agent_42', 'agent_43', 'agent_44', 'agent_45', 'agent_46', 'agent_47', 'agent_48', 'agent_49', 'agent_50'], 'origins': ['1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4'], 'destinations': ['2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3']}\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    traffic = traffic_f(high=False)\n",
    "    print(traffic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Road Network Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code defines a custom environment for simulating traffic assignment on the Nguyen network using the Gymnasium and PettingZoo libraries. \n",
    "The environment is designed for multi-agent reinforcement learning (MARL) scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium\n",
    "from gymnasium.spaces import Box, Dict, Discrete\n",
    "from gymnasium.spaces.utils import flatten_space\n",
    "from gymnasium.utils import EzPickle\n",
    "import numpy as np\n",
    "\n",
    "from pettingzoo import AECEnv\n",
    "from pettingzoo.utils import agent_selector, wrappers\n",
    "from pettingzoo.utils.conversions import parallel_wrapper_fn\n",
    "\n",
    "\n",
    "# allows to import the parallel environment using \"from NguyenNetworkEnv import parallel_env\"\n",
    "__all__ = [\"ManualPolicy\", \"env\", \"parallel_env\", \"raw_env\"]\n",
    "\n",
    "# environment wrapper\n",
    "def env(**kwargs):\n",
    "    env = raw_env(**kwargs)\n",
    "    env = wrappers.OrderEnforcingWrapper(env)\n",
    "    return env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Environment Initialization: Sets up the Nguyen Network environment with traffic demand from agents.\n",
    "\n",
    "* Observation Space: Defines the observation space for each agent, considering the network structure and latency information.\n",
    "\n",
    "* Action Space: Defines the action space for each agent, representing the choice of road routes.\n",
    "\n",
    "* Step Function: Simulates one step in the environment for the agents, updating their positions, rewards, and other state information.\n",
    "\n",
    "* Reset Function: Resets the environment to its initial state.\n",
    "\n",
    "* Additional Functions:\n",
    "  - observe(agent): Generates observations for a given agent based on its current position and neighbors.\n",
    "  - state(): Returns an array-like object for logging, containing termination triggers and agent travel times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AEC to parallel wrapper\n",
    "parallel_env = parallel_wrapper_fn(env)\n",
    "class raw_env(AECEnv):\n",
    "    metadata = {\n",
    "        \"name\": \"NguyenNet\",\n",
    "        \"is_parallelizable\": True\n",
    "    }\n",
    "    \n",
    "    \"\"\"\n",
    "    This is the traffic assignment environment. More documentation to follow.\n",
    "    \"\"\"\n",
    "    \n",
    "    # initialize environment with the Nguyen Network and Traffic Demand from agents\n",
    "    def __init__(\n",
    "        self,\n",
    "        net = nguyenNetwork(),\n",
    "        traffic = traffic_f(high=False),\n",
    "        render_mode = None\n",
    "        ):\n",
    "        \n",
    "        # no rendering at the moment\n",
    "        self.render_mode = render_mode\n",
    "        \n",
    "        # initialize network from Nguyen Network\n",
    "        self.road_network = net\n",
    "        self.traffic = traffic\n",
    "        \n",
    "        # initialize agents\n",
    "        self.agents = self.traffic[\"agents\"] # list of agents in environment\n",
    "        self.possible_agents = self.agents[:]\n",
    "        self.agent_name_mapping = dict(zip(self.agents, list(range(len(self.agents)))))\n",
    "        self._agent_selector = agent_selector(self.agents)\n",
    "        \n",
    "        # check\n",
    "        self.terminations_check = dict(zip(self.agents, [False for _ in self.agents]))\n",
    "        self.truncations_check = dict(zip(self.agents, [False for _ in self.agents]))\n",
    "        self.termination_trigger = dict(zip(self.agents, [False for _ in self.agents]))\n",
    "        self.truncation_trigger = dict(zip(self.agents, [False for _ in self.agents]))\n",
    "        \n",
    "        # agent origin, destination, and location information\n",
    "        self.agent_origins = self.traffic[\"origins\"].copy()\n",
    "        self.agent_origin_backup = self.traffic[\"origins\"].copy()\n",
    "        self.agent_locations = self.traffic[\"origins\"].copy()\n",
    "        self.agent_destinations = self.traffic[\"destinations\"]\n",
    "        self.agent_path_histories = {agent: [location] for agent, location in zip(self.agents, self.agent_locations)}\n",
    "        self.agent_wait_time = {agent: 0 for agent in self.agents}\n",
    "        self.agent_travel_time = {agent: 0 for agent in self.agents}\n",
    "\n",
    "        # agent unflattened observation space, this is flattened alphabetically btw.\n",
    "        self.unflattened_observation_spaces = {\n",
    "            agent: Dict({\n",
    "                \"observation\": Box(low=-1, high=12, shape=(2,1), dtype=int),\n",
    "                \"latencies\": Box(low=0, high=30, shape=(2,1), dtype=int),\n",
    "                \"location\": Box(low = 0, high = 12, shape = (1,1), dtype=int),\n",
    "            }) for agent in self.agents\n",
    "        }\n",
    "        \n",
    "        # agent flattened observatino space\n",
    "        self.observation_spaces = {\n",
    "            i: flatten_space(self.unflattened_observation_spaces[i]) for i in self.unflattened_observation_spaces\n",
    "        }\n",
    "        \n",
    "        # agent action space\n",
    "        self.action_spaces = dict(\n",
    "            # with the nguyen network agents have at most 2 choices\n",
    "            zip(\n",
    "                self.agents,\n",
    "                [gymnasium.spaces.Discrete(2)]*len(self.agents)\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # agent terminal and truncated states\n",
    "        self.terminate = False\n",
    "        self.truncate = False\n",
    "        \n",
    "    def observation_space(self, agent):\n",
    "        return self.observation_spaces[agent]\n",
    "    \n",
    "    def action_space(self, agent):\n",
    "        return self.action_spaces[agent]\n",
    "    \n",
    "    def observe(self, agent):\n",
    "\n",
    "        agent_idx = self.agent_name_mapping[agent]\n",
    "        \n",
    "        # get possible nodes the agent can travel to\n",
    "        agent_position = self.agent_locations[agent_idx]\n",
    "        agent_node_neighbors = list(self.road_network.neighbors(agent_position))\n",
    "        \n",
    "        # encode position\n",
    "        position = []\n",
    "        encoding = int(self.agent_locations[agent_idx])-1\n",
    "        position.append(encoding)\n",
    "        \n",
    "        # encode node positions\n",
    "        node_encoded = []\n",
    "        for node in agent_node_neighbors:\n",
    "            encoding = int(node)-1\n",
    "            node_encoded.append(encoding)\n",
    "        \n",
    "        # currently using ffs attribute, need to revise to use updated latency\n",
    "        # neighboring_nodes_ffs = []\n",
    "        # for node in agent_node_neighbors:\n",
    "        #     ffs_value = self.road_network.get_edge_data(agent_position, node)[\"ffs\"]\n",
    "        #     neighboring_nodes_ffs.append(ffs_value)\n",
    "\n",
    "        # updated to use \"latency\" instead of \"ffs\"\n",
    "        neighboring_nodes_latencies = []\n",
    "        for node in agent_node_neighbors:\n",
    "            latency_value = self.road_network.get_edge_data(agent_position, node)[\"latency\"]\n",
    "            neighboring_nodes_latencies.append(latency_value)\n",
    "\n",
    "        # return observation – a list in structured as [node1, latency1, node 2, latency2]\n",
    "        if len(agent_node_neighbors) == 1:\n",
    "            node_encoded = node_encoded*2\n",
    "            # neighboring_nodes_ffs = neighboring_nodes_ffs*2\n",
    "            neighboring_nodes_latencies = neighboring_nodes_latencies*2\n",
    "        if len(agent_node_neighbors) == 0:\n",
    "            node_encoded = [-1,-1]\n",
    "            # neighboring_nodes_ffs = [0,0]\n",
    "            neighboring_nodes_latencies = [0,0]\n",
    "            \n",
    "        # observations  = np.array(neighboring_nodes_ffs+position+node_encoded)\n",
    "        observations = np.array(neighboring_nodes_latencies + position + node_encoded)\n",
    "        \n",
    "        return observations\n",
    "\n",
    "\n",
    "\n",
    "    def state(self) -> np.ndarray:\n",
    "        \"\"\"We need to return an np-array like object for logging\"\"\"\n",
    "        return list(self.termination_trigger.values()), list(self.agent_travel_time.values())\n",
    "\n",
    "\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"This does not work\"\"\"\n",
    "        # # check if agent is dead\n",
    "        # if (\n",
    "        #     self.terminations_check[self.agent_selection] or\n",
    "        #     self.truncations_check[self.agent_selection]\n",
    "        # ):\n",
    "        #     # self.rewards[self.agent_selection] = 0\n",
    "        #     self.agent_selection = self._agent_selector.next()\n",
    "        #     #self._accumulate_rewards()\n",
    "        #     return\n",
    "        \n",
    "        action = np.asarray(action)\n",
    "        agent = self.agent_selection\n",
    "        agent_idx = self.agent_name_mapping[agent]        \n",
    "\n",
    "        self._cumulative_rewards[agent] = 0\n",
    "        \n",
    "        if self._agent_selector.is_last():\n",
    "            # Track the count of agents on each link for all agents' positions\n",
    "            agents_on_link = {edge: 0 for edge in self.road_network.edges()}\n",
    "            # print(agents_on_link)\n",
    "            # print(self.road_network.edges())\n",
    "\n",
    "            # Update the count based on all agents' neighbors\n",
    "            for current_agent in self.agents:                   \n",
    "                agent_idx = self.agent_name_mapping[current_agent]                \n",
    "                agent_position = self.agent_locations[agent_idx]                \n",
    "                agent_node_neighbors = list(self.road_network.neighbors(agent_position))\n",
    "\n",
    "                for node in agent_node_neighbors:                    \n",
    "                    for edge in zip([agent_position], [node]):                        \n",
    "                        agents_on_link[edge] += 1  # Increment count when an agent enters a link                                  \n",
    "\n",
    "            for edge, agent_count in agents_on_link.items():                \n",
    "                link_data = self.road_network.get_edge_data(*edge)  \n",
    "                # print(link_data) # not updated                                       \n",
    "\n",
    "                flow = agent_count  # 'flow' takes the total number of vehicles in the link                                         \n",
    "                start_node = np.int64(edge[0])            \n",
    "                end_node = np.int64(edge[1])            \n",
    "\n",
    "                new_latency = latency(flow, start_node, end_node)                       \n",
    "                link_data[\"latency\"] = new_latency            \n",
    "                # print(new_latency)                   \n",
    "\n",
    "                self.road_network[edge[0]][edge[1]][\"latency\"] = new_latency\n",
    "                # print(f\"Updated latency for edge {edge}: {self.road_network[edge[0]][edge[1]]['latency']}\") \n",
    "\n",
    "            self._clear_rewards()\n",
    "        else:\n",
    "            self._clear_rewards()\n",
    "            pass\n",
    "\n",
    "        # agent travel decrement\n",
    "        if self.agent_wait_time[agent] != 0:\n",
    "            # if agent has waiting time (i.e. \"traveling\" along edge, decrement wait time by one time step)\n",
    "            self.agent_wait_time[agent] -= 1\n",
    "            self.agent_selection = self._agent_selector.next()\n",
    "            self._accumulate_rewards()\n",
    "            return\n",
    "        \n",
    "        # agent reaches terminal state\n",
    "        if self.agent_locations[agent_idx] == self.agent_destinations[agent_idx]:\n",
    "            self.terminations_check[agent] = True\n",
    "            \n",
    "            # return reward for arriving at destionation\n",
    "            if self.termination_trigger[agent] == False:\n",
    "                completion_reward = 100\n",
    "                self.termination_trigger[agent] = True\n",
    "            else:\n",
    "                completion_reward = 0\n",
    "            \n",
    "            self.rewards[agent] = completion_reward\n",
    "            self.agent_selection = self._agent_selector.next()\n",
    "            self._accumulate_rewards()\n",
    "            return\n",
    "        \n",
    "        # agent reaches truncation state\n",
    "        if self.agent_locations[agent_idx] != self.agent_destinations[agent_idx] and \\\n",
    "        (self.agent_locations[agent_idx] == \"2\" or self.agent_locations[agent_idx] == \"3\"):\n",
    "            self.truncations_check[agent] = True\n",
    "                    \n",
    "            # return penalty for arriving at wrong destination\n",
    "            if self.truncation_trigger[agent] == False:\n",
    "                completion_penalty = -100\n",
    "                self.truncation_trigger[agent] = True\n",
    "            else:\n",
    "                completion_penalty = 0\n",
    "                \n",
    "            self.rewards[agent] = completion_penalty\n",
    "            self.agent_selection = self._agent_selector.next()\n",
    "            self._accumulate_rewards()\n",
    "            return\n",
    "        \n",
    "        # agent chooses action\n",
    "        choices =  list(\n",
    "            self.road_network.neighbors(\n",
    "                self.agent_locations[agent_idx]\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # if only one route\n",
    "        if len(choices) == 1:\n",
    "            choices = [choices[0], choices[0]]\n",
    "        \n",
    "        # select action\n",
    "        chosen_route = choices[action]\n",
    "\n",
    "        # reward based on chosen route latency, again using ffs instead of calculated latency, need a _calculate_reward(agent) method for this\n",
    "        reward = self.road_network.get_edge_data(\n",
    "            self.agent_locations[agent_idx],\n",
    "            # chosen_route)[\"ffs\"]\n",
    "            chosen_route)[\"latency\"]\n",
    "        \n",
    "        # add negative latency to reward – DQN to maximize negative reward\n",
    "        self.rewards[agent] = -1*reward\n",
    "        \n",
    "        # update latency\n",
    "        self.agent_wait_time[agent] += reward\n",
    "        self.agent_travel_time[agent] += reward        \n",
    "        \n",
    "        # update agent position\n",
    "        self.agent_locations[agent_idx] = chosen_route\n",
    "        \n",
    "        # update path history\n",
    "        self.agent_path_histories[agent].append(chosen_route)\n",
    "        \n",
    "        \n",
    "        # set the next agent to act\n",
    "        self.agent_selection = self._agent_selector.next()\n",
    "        self._accumulate_rewards()\n",
    "\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        # reset to initial states\n",
    "        self.agent_origins = self.agent_origin_backup.copy()\n",
    "        self.agent_locations = self.agent_origin_backup.copy()\n",
    "        self.agent_path_histories = {agent: [location] for agent, location in zip(self.agents, self.agent_origins)}\n",
    "        self.agent_wait_time = {agent: 0 for agent in self.agents}\n",
    "        self.agent_travel_time = {agent: 0 for agent in self.agents} # added\n",
    "\n",
    "        self.agents = self.possible_agents[:]        \n",
    "        self._agent_selector.reinit(self.agents)\n",
    "        self.agent_selection = self._agent_selector.next()\n",
    "        self.terminate = False\n",
    "        self.truncate = False\n",
    "        self.rewards = dict(zip(self.agents, [0 for _ in self.agents]))\n",
    "        self._cumulative_rewards = {a: 0 for a in self.agents}\n",
    "        self.terminations = dict(zip(self.agents, [False for _ in self.agents]))\n",
    "        self.truncations = dict(zip(self.agents, [False for _ in self.agents]))\n",
    "        self.infos = dict(zip(self.agents, [{} for _ in self.agents]))\n",
    "        \n",
    "        # Reset termination and truncation checks\n",
    "        # check\n",
    "        self.terminations_check = dict(zip(self.agents, [False for _ in self.agents]))\n",
    "        self.truncations_check = dict(zip(self.agents, [False for _ in self.agents]))\n",
    "        self.termination_trigger = dict(zip(self.agents, [False for _ in self.agents]))\n",
    "        self.truncation_trigger = dict(zip(self.agents, [False for _ in self.agents]))\n",
    "        \n",
    "        # Reset road network-related variables\n",
    "        # print(\"Resetting latency values\")\n",
    "        for edge in self.road_network.edges():\n",
    "            start_node, end_node = edge\n",
    "            self.road_network[start_node][end_node][\"latency\"] = 0            \n",
    "\n",
    "        # Clear any existing rewards\n",
    "        #self._clear_rewards()\n",
    "        # print(self.rewards)\n",
    "        # print(self.state())\n",
    "\n",
    "        # Return the initial state\n",
    "        #return self.state()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train agileRL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code implements a Multi-Agent Deep Deterministic Policy Gradients (MADDPG) algorithm to address traffic control in road networks, specifically the Nguyen Network. The environment is constructed using Gymnasium and PettingZoo libraries, and the MADDPG algorithm is employed to train a group of agents to navigate through the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import RoadNetEnv\n",
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "from agilerl.components.multi_agent_replay_buffer import MultiAgentReplayBuffer\n",
    "from agilerl.algorithms.maddpg import MADDPG\n",
    "\n",
    "# instantiate env and torch device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# env = RoadNetEnv.parallel_env()\n",
    "env = parallel_env()\n",
    "env.reset()\n",
    "\n",
    "# configure algo input parameters\n",
    "try:\n",
    "    state_dim = [env.observation_space(agent).n for agent in env.agents]\n",
    "    one_hot = True\n",
    "except Exception:\n",
    "    state_dim = [env.observation_space(agent).shape for agent in env.agents]\n",
    "    one_hot = False\n",
    "try:\n",
    "    action_dim = [env.action_space(agent).n for agent in env.agents]\n",
    "    discrete_actions = True\n",
    "    max_action = None\n",
    "    min_action = None\n",
    "except Exception:\n",
    "    action_dim = [env.action_space(agent).shape[0] for agent in env.agents]\n",
    "    discrete_actions = False\n",
    "    max_action = [env.action_space(agent).high for agent in env.agents]\n",
    "    min_action = [env.action_space(agent).low for agent in env.agents]\n",
    "\n",
    "n_agents = env.num_agents\n",
    "agent_ids = [agent_id for agent_id in env.agents]\n",
    "done = {agent_id: False for agent_id in env.agents}\n",
    "field_names = [\"state\", \"action\", \"reward\", \"next_state\", \"done\"]\n",
    "memory = MultiAgentReplayBuffer(\n",
    "    memory_size=1000000,\n",
    "    field_names=field_names,\n",
    "    agent_ids=agent_ids,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "with open('device_info.txt', 'w') as file:\n",
    "\tfile.write(f\"Device: {device}\")\n",
    "\n",
    "NET_CONFIG = {\n",
    "    \"arch\": \"mlp\",\n",
    "    \"h_size\": [128, 128]\n",
    "}\n",
    "\n",
    "agent = MADDPG(\n",
    "    state_dims=state_dim,\n",
    "    action_dims=action_dim,\n",
    "    one_hot=one_hot,\n",
    "    n_agents=n_agents,\n",
    "    agent_ids=agent_ids,\n",
    "    max_action=max_action,\n",
    "    min_action=min_action,\n",
    "    discrete_actions=True,\n",
    "    device=device,\n",
    "    net_config=NET_CONFIG,\n",
    "    lr=1e-2,\n",
    "    batch_size=8,\n",
    "    gamma=0.99,  # Discount Factor (Gamma, defaults to 0.95)\n",
    "    expl_noise=0.2,  # Exploration Noise (defaults to 0.1)\n",
    "\n",
    "    # lr=1e-3,  # Learning Rate for Optimizer (defaults to 0.01)\n",
    "    # batch_size=128, # Batch Size (defaults to 64)    \n",
    "    # tau=0.005, # Soft Update Parameter (Tau, defaults to 0.01)\n",
    "    # gamma=0.99,  # Discount Factor (Gamma, defaults to 0.95)\n",
    "    # expl_noise=0.2,  # Exploration Noise (defaults to 0.1)\n",
    "\n",
    ")\n",
    "\n",
    "\n",
    "# load checkpoint\n",
    "# agent.loadCheckpoint(\"./checkpoint/checkpoint_60.pt\")\n",
    "\n",
    "episodes = 200\n",
    "max_steps = 50\n",
    "epsilon = 1.0\n",
    "eps_end = 0.1\n",
    "eps_decay = 0.995\n",
    "\n",
    "episode_travel_times = []\n",
    "\n",
    "log_interval = 10\n",
    "start_ep = 1\n",
    "\n",
    "for ep in range(episodes):\n",
    "    state, info = env.reset()\n",
    "    agent_reward = {agent_id: 0 for agent_id in env.agents}\n",
    "    \n",
    "    for i in range(max_steps):\n",
    "        agent_mask = info[\"agent_mask\"] if \"agent_mask\" in info.keys() else None\n",
    "        env_defined_actions = (\n",
    "            info[\"env_defined_actions\"]\n",
    "            if \"env_defined_actions\" in info.keys()\n",
    "            else None\n",
    "        )\n",
    "        \n",
    "        # get next action from agent\n",
    "        cont_actions, discrete_action = agent.getAction(\n",
    "            state, epsilon, agent_mask, env_defined_actions\n",
    "        )\n",
    "        if agent.discrete_actions:\n",
    "            action = discrete_action\n",
    "        else:\n",
    "            action = cont_actions\n",
    "        \n",
    "        # act in environment\n",
    "        next_state, reward, termination, truncation, info = env.step(\n",
    "            action\n",
    "        )\n",
    "        \n",
    "        # save experience to replay buffer\n",
    "        memory.save2memory(state, cont_actions, reward, next_state, done)\n",
    "        \n",
    "        for agent_id, r in reward.items():\n",
    "            agent_reward[agent_id] += r\n",
    "        \n",
    "        # learn according to learning frequency\n",
    "        if (memory.counter % agent.learn_step == 0) and (len(memory) >= agent.batch_size):\n",
    "            experiences = memory.sample(agent.batch_size)\n",
    "            agent.learn(experiences)\n",
    "            \n",
    "        # update state\n",
    "        state = next_state\n",
    "\n",
    "    # metric logging\n",
    "    finishers, travel_time = env.state()\n",
    "    episode_travel_times.append(travel_time) # export to csv\n",
    "    \n",
    "    # save the total episode reward\n",
    "    score = sum(agent_reward.values())\n",
    "    agent.scores.append(score)\n",
    "    \n",
    "    \n",
    "    # update epsilon for exploration\n",
    "    epsilon = max(eps_end, epsilon * eps_decay)\n",
    "\n",
    "    if (ep+start_ep) % log_interval == 0 or ep == episodes -1:\n",
    "        reward_scores = pd.DataFrame(agent.scores)\n",
    "        reward_scores.to_csv(f\"./results/reward_{ep+start_ep}.csv\")\n",
    "        travel_times = pd.DataFrame(episode_travel_times)\n",
    "        travel_times.to_csv(f\"./results/travel_times_{ep+start_ep}.csv\")\n",
    "        agent.saveCheckpoint(f\"./checkpoint/checkpoint_{ep+start_ep}.pt\")  \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rllib",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
